## 深度学习预备知识

**特征**

**样本**

**数据集**



**监督学习算法**：有标准答案，受答案（标签）监控

**无监督学习算法**：无标准答案

**半监督学习**：是前两种的结合方法，一部分有标签，一部分没有标签，有标签的对无标签的打标签，就变成了有标签的样本

**强化学习**：比较火热，会和环境不断交互，对结果进行评判，进行的好奖励，不好惩罚

**凹凸函数**：为了引出损失函数，来更好的优化

**凸集**：连接任意两点，线都在集合里

**凹集**：连接任意两点，线有一部分在外面

**凸函数**：连接函数两个点，他的值比线高

**凹函数**：连接函数两个点，他的值比线低

凸优化的定义：

1. 条件一：约束条件为凸集
2. 条件二：目标函数为凸集。

非凸优化问题转化为凸优化问题的方法：

1. 修改目标函数
2. 抛弃一些约束条件

**损失函数**：

神经网络中使用的代价函数被称作为损失函数

顺势函数**衡量了评分函数的预测与正式样本标签的吻合度**

Loss的值都会设置为和吻合程度**负相关**。如果算法公式是正相关，定义损失函数时候加负号，调整为负相关。

交叉熵损失函数

梯度下降

1. 批量梯度下降（BGD）：使用所有数据，但是当样本数量过多，更新速度会很低
2. 随机梯度下降（SGD）：每次更新会只考虑一个样本点，但很有可能只达到局部最优
3. 小批量梯度下降（MBGD）：解决了上面两种，每次只用一部分来更新

学习率：根据误差梯度调整权重值的系数



## 人工神经网络

#### 神经网络介绍和神经元

**生物神经网络**

**人工神经网**络简称神经网络**ANN**

是指模仿人脑结构及其功能的信息处理系统

**神经元**

​	线性函数+激活函数

要考虑

**拓扑结构**：如何连接

**激活规则**：

**学习算法**：如何训练数据

拓扑结构分为：

1. 前馈网络：一个方向进行传播，没有反向，下一层的输入就是上一层的输出
2. 反馈网络：经典的RNN，接收上一层的输出，也可以接收自己的数据，有更强的记忆能力
3. 图网络：定义在图结构的一个数据网络，节点可以有向也可以无向，是前馈与反馈的范化



#### 感知机

1957年 美国 Frank Rossenblatt提出了感知机

可以用来接收多个信号，输出一个信号

感知器的计算方法

激活函数

损失函数：即期望使误分类的所有样本，到超平面的距离只和最小

使用方法：

随机梯度下降

小批量梯度下降

感知机选择的是随机梯度下降，这意味着我们每次仅仅需要使用一个误分类点来更新梯度

更新原则

如果预测准确，则权重不进行更新，否则，增加权重，使其更趋向于正确的类别

感知机步骤

XOR问题

#### 激活函数

**sigmoid**

单调连续，求导容易

输出有界，网络比较收敛，

远离中心点区域0 ，容易产生梯度消失

**tanh**

随机梯度下降，从而降低迭代次数

远离中心点区域倒数趋于0 

**ReLU**

常用

修正线性单元

比较容易死亡

不够平滑，在回归问题就不够平滑

有效解决了梯度消失问题

**softmax**

是将一个k维的任意实数向量映射成另一个k维的实数向量



激活函数设计需考虑的因素

1. 非线性
2. 连续可微性
3. 有界性
4. 单调性
5. 平滑性

## 深度前馈网络

## 反向传播

## 神经网络的架构设计